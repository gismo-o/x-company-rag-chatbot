import streamlit as st
import google.generativeai as genai
import os
import pandas as pd
from PyPDF2 import PdfReader
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains.question_answering import load_qa_chain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import gspread
import json
from google.oauth2.service_account import Credentials




@st.cache_data # Bu konfigÃ¼rasyonun sadece bir kez okunmasÄ±nÄ± saÄŸlar
def load_csv_configs():
    """csv_configs.json dosyasÄ±nÄ± yÃ¼kler."""
    try:
        with open("csv_configs.json", "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        # EÄŸer konfigÃ¼rasyon dosyasÄ± yoksa, sadece varsayÄ±lan bir yapÄ± dÃ¶ndÃ¼r
        return {
            "_default": {
                "template": "{__ALL_COLUMNS__}."
            }
        }
    
def process_row_with_config(row: pd.Series, file_name: str, configs: dict) -> str:
    """
    Verilen bir CSV satÄ±rÄ±nÄ±, konfigÃ¼rasyon dosyasÄ±na gÃ¶re bir cÃ¼mleye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    # Dosya adÄ± iÃ§in Ã¶zel bir konfigÃ¼rasyon var mÄ± kontrol et, yoksa _default kullan
    config = configs.get(file_name, configs["_default"])
    template = config["template"]
    
    # SatÄ±r verilerini bir sÃ¶zlÃ¼ÄŸe Ã§evir 
    row_dict = row.to_dict()

    # Åablondaki Ã¶zel yer tutucularÄ± iÅŸle
    if "{__COLUMN_0__}" in template:
        template = template.replace("{__COLUMN_0__}", str(row.iloc[0]))
    
    if "{__ALL_COLUMNS__}" in template:
        all_cols_text = ", ".join([f"{k}: {v}" for k, v in row_dict.items() if pd.notna(v)])
        template = template.replace("{__ALL_COLUMNS__}", all_cols_text)

    # Geri kalan tÃ¼m normal {SÃ¼tunAdÄ±} yer tutucularÄ±nÄ± doldur
    # .format(**row_dict) metodu, sÃ¶zlÃ¼kteki anahtarlarla ÅŸablondaki yer tutucularÄ± eÅŸleÅŸtirir
    try:
        return template.format(**row_dict)
    except KeyError as e:
        print(f"Åablonda hata: CSV'de olmayan bir sÃ¼tun isteniyor -> {e}")
        return "" # Hata durumunda boÅŸ string dÃ¶ndÃ¼r



# TÃ¼m dÃ¶kÃ¼manlardan metinleri Ã§Ä±karan ana fonksiyon
def get_documents_text():
    # Ä°lk olarak CSV konfigÃ¼rasyonunu yÃ¼kle
    csv_configs = load_csv_configs()

    docs_path = "./data/"
    if not os.path.exists(docs_path):
        return ""
        
    file_paths = [os.path.join(docs_path, f) for f in os.listdir(docs_path)]
    raw_text = ""

    # PDF'leri iÅŸle
    for path in filter(lambda p: p.endswith('.pdf'), file_paths):
        try:
            reader = PdfReader(path)
            for page in reader.pages:
                raw_text += page.extract_text() or ""
        except Exception as e:
            print(f"PDF okuma hatasÄ± {path}: {e}")

    # CSV'leri iÅŸle
    for path in filter(lambda p: p.endswith('.csv'), file_paths):
        try:
            file_name = os.path.basename(path)
            df = pd.read_csv(path, sep=';', on_bad_lines='skip')
            for index, row in df.iterrows():
                sentence = process_row_with_config(row, file_name, csv_configs)
                raw_text += sentence + "\n"

        except Exception as e:
            print(f"CSV okuma hatasÄ± {path}: {e}")
            
    return raw_text

# Metni yÃ¶netilebilir parÃ§alara bÃ¶len fonksiyon
def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=10000, 
        chunk_overlap=1000
    )
    chunks = text_splitter.split_text(text)
    return chunks


# Metin parÃ§alarÄ±ndan vektÃ¶r deposu oluÅŸturan ve kaydeden fonksiyon
def get_vector_store(text_chunks):
    # Google'Ä±n embedding modelini yÃ¼kle
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    
    # Chroma DB'yi diskte saklamak iÃ§in bir dizin belirt. Bu sayede her seferinde baÅŸtan oluÅŸturmak gerekmez
    vector_store_path = "chroma_db"
    
    # EÄŸer daha Ã¶nce oluÅŸturulmuÅŸ bir veritabanÄ± varsa, onu yÃ¼kle
    if os.path.exists(vector_store_path):
        vector_store = Chroma(persist_directory=vector_store_path, embedding_function=embeddings)
    # Yoksa, metin parÃ§alarÄ±ndan yeni bir veritabanÄ± oluÅŸtur
    else:
        vector_store = Chroma.from_texts(
            text_chunks, 
            embedding=embeddings, 
            persist_directory=vector_store_path
        )
        vector_store.persist()
        
    return vector_store


# LLM ile konuÅŸma zincirini oluÅŸturan fonksiyon
def get_conversational_chain():
    # Prompt ÅŸablonu: LLM'e nasÄ±l davranmasÄ± gerektiÄŸini sÃ¶ylÃ¼yoruz
    prompt_template = """
    Sen X-Company'nin yardÄ±msever bir kurumsal asistanÄ±sÄ±n. CevaplarÄ±nÄ± sadece aÅŸaÄŸÄ±da verilen baÄŸlama (context) dayanarak, kÄ±sa ve Ã¶z bir ÅŸekilde oluÅŸtur. 
    EÄŸer cevap verilen baÄŸlamda bulunmuyorsa, "Bu konuda bilgi sahibi deÄŸilim." de. Kendi bilgini kullanma.

    Context:
    {context}

    Question:
    {question}

    Answer:
    """
    
    # LLM modelini yapÄ±landÄ±r
    model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3)
    
    # Prompt'u ve modeli bir araya getir
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    
    # Soru-cevap zincirini yÃ¼kle
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    
    return chain


# KullanÄ±cÄ± sorusunu iÅŸleyen ana fonksiyon
def handle_user_input(user_question, vector_store):
    # KullanÄ±cÄ±nÄ±n sorusuna en Ã§ok benzeyen dÃ¶kÃ¼manlarÄ± vektÃ¶r deposunda bul
    # k=5, en alakalÄ± 5 metin parÃ§asÄ±nÄ± getirmesini sÃ¶yler.
    docs = vector_store.similarity_search(user_question, k=5)
    
    # Soru-cevap zincirini al
    chain = get_conversational_chain()
    
    # Zinciri dÃ¶kÃ¼manlar ve soru ile Ã§alÄ±ÅŸtÄ±r
    response = chain(
        {"input_documents": docs, "question": user_question}, 
        return_only_outputs=True
    )
    
    return response["output_text"]



# IT SÄ±nÄ±flandÄ±rma modelini ve tokenizer'Ä± yÃ¼kleyen fonksiyon
@st.cache_resource
def load_classification_model():
    """IT sÄ±nÄ±flandÄ±rma modelini ve tokenizer'Ä± Hugging Face Hub'dan yÃ¼kler."""
    model_path = "gismo-o/x-company-it-ticket-classifier" #Hugging Face Yolu
    
    print(f"IT sÄ±nÄ±flandÄ±rma modeli {model_path} adresinden yÃ¼kleniyor...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        print("Model baÅŸarÄ±yla yÃ¼klendi.")
        return tokenizer, model
    except Exception as e:
        # Hata durumunda Streamlit arayÃ¼zÃ¼nde net bir hata mesajÄ± gÃ¶ster
        st.error(
            f"Hugging Face Hub'dan model yÃ¼klenirken bir hata oluÅŸtu: {e}\n\n"
            f"LÃ¼tfen kontrol edin:\n"
            f"1. Model reposunun adÄ± doÄŸru mu? ('{model_path}')\n"
            f"2. Repo 'public' olarak ayarlÄ± mÄ±?\n"
            f"3. Ä°nternet baÄŸlantÄ±nÄ±zda bir sorun var mÄ±?"
        )
        # Hata durumunda uygulamanÄ±n devam etmesini engelle
        return None, None

# KullanÄ±cÄ±nÄ±n girdisini sÄ±nÄ±flandÄ±ran fonksiyon
def predict_it_ticket_category(text, tokenizer, model):
    # Metni token'lara Ã§evir
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    
    # Modelden tahmin al
    with torch.no_grad():
        logits = model(**inputs).logits
        
    # OlasÄ±lÄ±klarÄ± hesapla
    probabilities = torch.nn.functional.softmax(logits, dim=-1).flatten()
    
    # En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip etiketi ve skorunu bul
    predicted_class_id = probabilities.argmax().item()
    confidence = probabilities[predicted_class_id].item()
    predicted_class_label = model.config.id2label[predicted_class_id]
    
    return predicted_class_label, confidence


# Google Sheets'e destek talebini kaydeden fonksiyon
def save_ticket_to_gsheet(konu, detay, aciliyet, kategori):
    try:
        # Streamlit'in secrets yÃ¶netiminden kimlik bilgilerini al
        scopes = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds = Credentials.from_service_account_info(
            st.secrets["gcp_service_account"], scopes=scopes
        )
        client = gspread.authorize(creds)
        
        # Google Sheet'i adÄ±yla aÃ§ ve ilk Ã§alÄ±ÅŸma sayfasÄ±nÄ± seÃ§
        sheet = client.open("X-Company IT Talepleri").sheet1
        
        # Yeni satÄ±r olarak eklenecek veriyi hazÄ±rla
        new_row = [
            pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            kategori,
            konu,
            detay,
            aciliyet,
            "Yeni Talep"
        ]
        sheet.append_row(new_row)
        return True
    except Exception as e:
        st.error(f"VeritabanÄ±na yazÄ±lÄ±rken bir hata oluÅŸtu: {e}")
        return False


# IT sorunu olarak kabul edilecek kategorilerin listesi
IT_CATEGORIES = [
    "AÄŸ", "DonanÄ±m", "YazÄ±lÄ±m", "Åifre", "YazÄ±cÄ± Sorunu", "DonanÄ±m AÄŸÄ±",
    "VPN", "Email", "Veri ve Dosya YÃ¶netimi", "Sistem GÃ¼ncellemeleri",
    "YazÄ±cÄ± / TarayÄ±cÄ± / Periferik", "Web ve Uygulama EriÅŸimi",
    "GÃ¼venlik ve Antivirus", "Ses ve GÃ¶rÃ¼ntÃ¼", "Hesap ve Yetki",
    "ToplantÄ± / Video Konferans"
] 


# Streamlit uygulamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±ran ana fonksiyon
def main():
    load_dotenv()
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

    st.set_page_config(
        page_title="X-Company Kurumsal Asistan", 
        page_icon=":robot_face:"
    )

    # VERÄ° Ä°ÅLEME VE MODELLERÄ° YÃœKLEME 
    @st.cache_resource
    def load_rag_vector_store():
        print("RAG VeritabanÄ± yÃ¼kleniyor...")
        raw_text = get_documents_text()
        text_chunks = get_text_chunks(raw_text)
        vector_store = get_vector_store(text_chunks)
        return vector_store
    
    rag_vector_store = load_rag_vector_store()
    it_tokenizer, it_model = load_classification_model()

    # ARAYÃœZ ELEMENTLERÄ°
    st.header("ğŸ’¬ X-Company Dijital Asistan")

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": (
            "Merhaba! Ben **X-Company AsistanÄ±yÄ±m**. "
            "Åirket sÃ¼reÃ§leri, bilgiler veya genel sorularÄ±nÄ±z iÃ§in buradayÄ±m. "
            "BugÃ¼n size nasÄ±l yardÄ±mcÄ± olabilirim?"
        )}
        ]

    # Sohbet geÃ§miÅŸini gÃ¶ster 
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"], unsafe_allow_html=True)


    # Formun durumunu chat girdisi dÄ±ÅŸÄ±nda yÃ¶netme
    # EÄŸer bir Ã¶nceki adÄ±mda form gÃ¶nderildiyse, baÅŸarÄ± mesajÄ±nÄ± gÃ¶ster.
    # Bu, sayfa yenilense bile mesajÄ±n kalÄ±cÄ± olmasÄ±nÄ± saÄŸlar.
    if st.session_state.get("ticket_submitted", False):
        st.success("Destek talebiniz baÅŸarÄ±yla IT ekibine iletildi!")
        st.session_state.show_form = False  # Formu tekrar gÃ¶sterme
        del st.session_state["ticket_submitted"] # BayraÄŸÄ± temizle

    # Yeni bir chat girdisi varsa, onu iÅŸle ve state'i gÃ¼ncelle.
    if prompt := st.chat_input("Sorunuzu buraya yazabilirsiniz..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Yeni bir mesaj geldiÄŸinde, Ã¶nceki form durumunu temizle
        st.session_state.show_form = False
        
        with st.chat_message("assistant"):
            with st.spinner("YanÄ±t hazÄ±rlanÄ±yor, lÃ¼tfen bekleyin..."):
                category, confidence = predict_it_ticket_category(prompt, it_tokenizer, it_model)
                #st.info(f"DEBUG: Tahmin: '{category}', GÃ¼venilirlik: {confidence:.2f}")

                is_it_category = category in IT_CATEGORIES and confidence > 0.59

                if is_it_category:
                    # Formu gÃ¶stermek iÃ§in sadece bir "bayrak" ayarla.
                    st.session_state.show_form = True
                    # Formun ihtiyaÃ§ duyacaÄŸÄ± bilgileri state'e kaydet
                    st.session_state.current_category = category
                    st.session_state.current_prompt = prompt
                else:
                    # IT sorunu deÄŸilse, RAG ile cevap ver ve bitir.
                    response = handle_user_input(prompt, rag_vector_store)
                    st.markdown(response)
                    st.session_state.messages.append({"role": "assistant", "content": response})

    # Bayrak True ise, formu gÃ¶ster. Bu bÃ¶lÃ¼m her etkileÅŸimde kontrol edilir.
    if st.session_state.get("show_form", False):
        # State'den gerekli bilgileri al
        category = st.session_state.current_category
        prompt_val = st.session_state.current_prompt

        st.warning(f"â„¹ï¸ AnlaÅŸÄ±lan bir **{category}** konusuyla karÅŸÄ± karÅŸÄ±yasÄ±nÄ±z. Ä°lgili formu doldurarak bize iletebilirsiniz.")
        
        with st.expander("IT Destek Formu", expanded=True):
            st.text_input("Konu:", value=prompt_val, disabled=True, key="ticket_konu")
            st.text_area("Sorunun DetaylarÄ±:", placeholder="LÃ¼tfen daha fazla detay verin...", key="ticket_detay")
            st.selectbox("Aciliyet Seviyesi:", ["DÃ¼ÅŸÃ¼k", "Normal", "YÃ¼ksek", "Kritik"], key="ticket_aciliyet")

            if st.button("Destek Talebi GÃ¶nder"):
                with st.spinner("Talebiniz gÃ¶nderiliyor..."):
                    success = save_ticket_to_gsheet(
                        st.session_state.ticket_konu,
                        st.session_state.ticket_detay,
                        st.session_state.ticket_aciliyet,
                        category
                    )
                    
                    if success:
                        # BaÅŸarÄ±lÄ± olursa, bir sonraki Ã§alÄ±ÅŸtÄ±rmada mesajÄ± gÃ¶stermek iÃ§in bayraÄŸÄ± ayarla
                        st.session_state.ticket_submitted = True
                        # DeÄŸiÅŸikliklerin anÄ±nda gÃ¶rÃ¼nmesi iÃ§in sayfayÄ± yeniden Ã§alÄ±ÅŸtÄ±r
                        st.rerun()
                    # BaÅŸarÄ±sÄ±z olursa, save_ticket_to_gsheet fonksiyonu zaten hata mesajÄ± gÃ¶sterecektir.

# UygulamayÄ± baÅŸlatmak iÃ§in
if __name__ == "__main__":
    main()