# -*- coding: utf-8 -*-
"""IT-tickets

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k0YmMMx2l4LYlW9Dy3loymGN_9i3DqwY
"""

import os
import random
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from datasets import Dataset, ClassLabel
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding
import evaluate # Import the evaluate library

# AYARLAR-
CSV_PATH = "it_tickets_dataset_3000.csv"   # üretilen CSV
MODEL_NAME = "dbmdz/bert-base-turkish-cased"
OUTPUT_DIR = "./it_ticket_model"
RANDOM_SEED = 42
NUM_EPOCHS = 3
BATCH_SIZE = 8
LEARNING_RATE = 2e-5
WEIGHT_DECAY = 0.01
MAX_LENGTH = 128


def set_seed(seed=RANDOM_SEED):
    random.seed(seed)
    np.random.seed(seed)
    try:
        import torch
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    except Exception:
        pass

set_seed()

# CSV yükle ve label encode et
df = pd.read_csv(CSV_PATH)
# Kolon isimleri: 'text' ve 'category'
assert "text" in df.columns and "category" in df.columns, "CSV 'text' ve 'category' kolonlarına sahip olmalı."

# Label encode
le = LabelEncoder()
df["label"] = le.fit_transform(df["category"])
label2id = {label: idx for idx, label in enumerate(le.classes_)}
id2label = {v: k for k, v in label2id.items()}
num_labels = len(le.classes_)
print(f"Labels ({num_labels}):", le.classes_)

# HuggingFace Dataset oluştur ve train/val/test split
dataset = Dataset.from_pandas(df[["text", "label"]])
dataset = dataset.train_test_split(test_size=0.15, seed=RANDOM_SEED)
test_valid = dataset["test"].train_test_split(test_size=0.5, seed=RANDOM_SEED)
raw_datasets = {
    "train": dataset["train"],
    "validation": test_valid["train"],
    "test": test_valid["test"]
}
print({k: len(v) for k,v in raw_datasets.items()})

# Tokenizer ve model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=num_labels, id2label=id2label, label2id=label2id
)

# Tokenization
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=False, max_length=MAX_LENGTH)

tokenized_datasets = {k: raw_datasets[k].map(preprocess_function, batched=True) for k in raw_datasets}

# Data collator for dynamic padding
data_collator = DataCollatorWithPadding(tokenizer)

# Metrics
accuracy = evaluate.load("accuracy")
f1 = evaluate.load("f1")
precision = evaluate.load("precision")
recall = evaluate.load("recall")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)

    acc = accuracy.compute(predictions=preds, references=labels)["accuracy"]
    f1_macro = f1.compute(predictions=preds, references=labels, average="macro")["f1"]
    precision_macro = precision.compute(predictions=preds, references=labels, average="macro", zero_division=0)["precision"]
    recall_macro = recall.compute(predictions=preds, references=labels, average="macro", zero_division=0)["recall"]
    return {"accuracy": acc, "f1_macro": f1_macro, "precision": precision_macro, "recall": recall_macro}

# TrainingArguments
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=NUM_EPOCHS,
    weight_decay=WEIGHT_DECAY,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    logging_steps=50,
    fp16=True if (os.environ.get("USE_FP16","1")=="1") else False,
    push_to_hub=False,
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Evaluate on test
metrics = trainer.evaluate(tokenized_datasets["test"])
print("Test metrics:", metrics)

# Save model, tokenizer, label encoder
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# save label encoder mapping
import json
with open(os.path.join(OUTPUT_DIR, "label2id.json"), "w", encoding="utf-8") as f:
    json.dump(label2id, f, ensure_ascii=False, indent=2)
with open(os.path.join(OUTPUT_DIR, "id2label.json"), "w", encoding="utf-8") as f:
    json.dump(id2label, f, ensure_ascii=False, indent=2)

print("Model ve tokenizer kaydedildi:", OUTPUT_DIR)

!zip -r /content/it_ticket_model.zip /content/it_ticket_model

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/it_ticket_model /content/drive/MyDrive/

!pip install --upgrade transformers datasets accelerate evaluate

!pip install evaluate